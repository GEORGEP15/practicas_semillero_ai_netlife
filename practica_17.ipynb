{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f3b814",
   "metadata": {},
   "source": [
    "# Taller: Memoria Conversacional en LLMs con LangChain\n",
    "\n",
    "## üß† M√≥dulo 1: Tipos de Memoria Conversacional\n",
    "\n",
    "En este taller aprender√°s a implementar diferentes tipos de memoria conversacional usando LangChain:\n",
    "\n",
    "- üìù **ConversationBufferMemory**: Memoria completa sin l√≠mites\n",
    "- ü™ü **ConversationBufferWindowMemory**: Ventana deslizante de mensajes recientes\n",
    "- üìã **ConversationSummaryMemory**: Res√∫menes autom√°ticos de conversaciones\n",
    "\n",
    "### Objetivos de Aprendizaje\n",
    "\n",
    "‚úÖ Entender c√≥mo funciona cada tipo de memoria\n",
    "\n",
    "‚úÖ Implementar chatbots con memoria persistente\n",
    "\n",
    "‚úÖ Comparar el uso de tokens entre diferentes tipos\n",
    "\n",
    "‚úÖ Aplicar la memoria adecuada seg√∫n el caso de uso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cadc32",
   "metadata": {},
   "source": [
    "## Paso 1: Instalaci√≥n de Librer√≠as üì¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ee386e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.12/dist-packages (2.1.12)\n",
      "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.31)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.80)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.13)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/lib/python3/dist-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: google-ai-generativelanguage<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (0.9.0)\n",
      "Requirement already satisfied: filetype<2,>=1.2 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.2.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.14)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.2.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.26.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.40.3)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.76.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (6.33.1)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /root/.local/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.11.12)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.71.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.76.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (4.9.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/lib/python3/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (0.4.8)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-google-genai langchain-community --break-system-packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca9a9d8",
   "metadata": {},
   "source": [
    "## Paso 2: Importar Librer√≠as üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28435850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Librer√≠as importadas correctamente!\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationBufferWindowMemory,\n",
    "    ConversationSummaryMemory\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8501a534",
   "metadata": {},
   "source": [
    "## Paso 3: Configurar API Key üîë\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANTE**: Reemplaza con tu propia API key de Google Gemini\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d80bc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API Key configurada\n"
     ]
    }
   ],
   "source": [
    "# üîê Configura tu API key aqu√≠\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDtpQBIBAfNCq4ioXy_Bw4QP5XbclpPF4I\"\n",
    "print(\"‚úÖ API Key configurada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eafa10",
   "metadata": {},
   "source": [
    "## Paso 4: Crear Modelo de Lenguaje (LLM) ü§ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f51ef1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM configurado (Gemini 2.5 flash)\n"
     ]
    }
   ],
   "source": [
    "# Configurar el modelo Gemini\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.7,\n",
    "    convert_system_message_to_human=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LLM configurado (Gemini 2.5 flash)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257e15bd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìù EJERCICIO 1: ConversationBufferMemory\n",
    "\n",
    "## ¬øQu√© es?\n",
    "\n",
    "**ConversationBufferMemory** almacena **TODOS los mensajes** de la conversaci√≥n sin l√≠mites.\n",
    "\n",
    "### Caracter√≠sticas:\n",
    "- ‚úÖ Mantiene el historial completo\n",
    "- ‚úÖ F√°cil de implementar\n",
    "- ‚ö†Ô∏è Puede crecer indefinidamente\n",
    "- ‚ö†Ô∏è Alto consumo de tokens en conversaciones largas\n",
    "\n",
    "### Caso de Uso Ideal:\n",
    "Conversaciones **cortas** donde necesitas todo el contexto (ej: asistente de 5-10 mensajes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecde1701",
   "metadata": {},
   "source": [
    "## Paso 5: Implementar ConversationBufferMemory üìù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16f1fd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ConversationBufferMemory creada\n",
      "üìù Tipo: Almacena TODOS los mensajes sin l√≠mite\n"
     ]
    }
   ],
   "source": [
    "# Crear memoria tipo Buffer (guarda todo)\n",
    "buffer_memory = ConversationBufferMemory(\n",
    "    memory_key=\"history\",  # IMPORTANTE: debe coincidir con el prompt\n",
    "    input_key=\"input\"\n",
    ")\n",
    "\n",
    "# Crear prompt template\n",
    "buffer_prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"],\n",
    "    template=\"\"\"\n",
    "Eres un asistente amigable y conversacional.\n",
    "\n",
    "Historial de conversaci√≥n:\n",
    "{history}\n",
    "\n",
    "Usuario: {input}\n",
    "Asistente:\"\"\"\n",
    ")\n",
    "\n",
    "# Crear cadena conversacional\n",
    "conversation_buffer = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=buffer_prompt,\n",
    "    memory=buffer_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ConversationBufferMemory creada\")\n",
    "print(\"üìù Tipo: Almacena TODOS los mensajes sin l√≠mite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995396dc",
   "metadata": {},
   "source": [
    "## Paso 6: Probar ConversationBufferMemory üß™\n",
    "\n",
    "Vamos a tener una conversaci√≥n de varios turnos y ver c√≥mo se guarda todo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd13a9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîµ PRUEBA 1: ConversationBufferMemory\n",
      "============================================================\n",
      "\n",
      "üí¨ Usuario: Hola, me llamo Ana\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Eres un asistente amigable y conversacional.\n",
      "\n",
      "Historial de conversaci√≥n:\n",
      "\n",
      "\n",
      "Usuario: Hola, me llamo Ana\n",
      "Asistente:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ü§ñ Asistente: ¬°Hola, Ana! Es un placer conocerte. Soy tu asistente virtual.\n",
      "\n",
      "¬øEn qu√© puedo ayudarte hoy?\n",
      "\n",
      "üí¨ Usuario: ¬øCu√°l es mi nombre?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Eres un asistente amigable y conversacional.\n",
      "\n",
      "Historial de conversaci√≥n:\n",
      "Human: Hola, me llamo Ana\n",
      "AI: ¬°Hola, Ana! Es un placer conocerte. Soy tu asistente virtual.\n",
      "\n",
      "¬øEn qu√© puedo ayudarte hoy?\n",
      "\n",
      "Usuario: ¬øCu√°l es mi nombre?\n",
      "Asistente:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ü§ñ Asistente: ¬°Tu nombre es Ana! Me lo dijiste hace un momento. üòä\n",
      "\n",
      "üí¨ Usuario: Me gusta la m√∫sica rock\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Eres un asistente amigable y conversacional.\n",
      "\n",
      "Historial de conversaci√≥n:\n",
      "Human: Hola, me llamo Ana\n",
      "AI: ¬°Hola, Ana! Es un placer conocerte. Soy tu asistente virtual.\n",
      "\n",
      "¬øEn qu√© puedo ayudarte hoy?\n",
      "Human: ¬øCu√°l es mi nombre?\n",
      "AI: ¬°Tu nombre es Ana! Me lo dijiste hace un momento. üòä\n",
      "\n",
      "Usuario: Me gusta la m√∫sica rock\n",
      "Asistente:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ü§ñ Asistente: ¬°Excelente gusto, Ana! El rock es un g√©nero con much√≠sima energ√≠a e historia. ü§ò\n",
      "\n",
      "¬øHay alguna banda o subg√©nero que te guste m√°s en particular?\n",
      "\n",
      "üí¨ Usuario: ¬øQu√© te dije sobre mis gustos?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Eres un asistente amigable y conversacional.\n",
      "\n",
      "Historial de conversaci√≥n:\n",
      "Human: Hola, me llamo Ana\n",
      "AI: ¬°Hola, Ana! Es un placer conocerte. Soy tu asistente virtual.\n",
      "\n",
      "¬øEn qu√© puedo ayudarte hoy?\n",
      "Human: ¬øCu√°l es mi nombre?\n",
      "AI: ¬°Tu nombre es Ana! Me lo dijiste hace un momento. üòä\n",
      "Human: Me gusta la m√∫sica rock\n",
      "AI: ¬°Excelente gusto, Ana! El rock es un g√©nero con much√≠sima energ√≠a e historia. ü§ò\n",
      "\n",
      "¬øHay alguna banda o subg√©nero que te guste m√°s en particular?\n",
      "\n",
      "Usuario: ¬øQu√© te dije sobre mis gustos?\n",
      "Asistente:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ü§ñ Asistente: Me dijiste que te gusta la m√∫sica rock. ¬°Un excelente gusto! ü§ò\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print(\"=\"*60)\n",
    "print(\"üîµ PRUEBA 1: ConversationBufferMemory\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Mensaje 1\n",
    "print(\"\\nüí¨ Usuario: Hola, me llamo Ana\")\n",
    "response1 = conversation_buffer.predict(input=\"Hola, me llamo Ana\")\n",
    "print(f\"ü§ñ Asistente: {response1}\")\n",
    "time.sleep(2)\n",
    "# Mensaje 2\n",
    "print(\"\\nüí¨ Usuario: ¬øCu√°l es mi nombre?\")\n",
    "response2 = conversation_buffer.predict(input=\"¬øCu√°l es mi nombre?\")\n",
    "print(f\"ü§ñ Asistente: {response2}\")\n",
    "time.sleep(2)\n",
    "# Mensaje 3\n",
    "print(\"\\nüí¨ Usuario: Me gusta la m√∫sica rock\")\n",
    "response3 = conversation_buffer.predict(input=\"Me gusta la m√∫sica rock\")\n",
    "print(f\"ü§ñ Asistente: {response3}\")\n",
    "time.sleep(2)\n",
    "# Mensaje 4\n",
    "print(\"\\nüí¨ Usuario: ¬øQu√© te dije sobre mis gustos?\")\n",
    "response4 = conversation_buffer.predict(input=\"¬øQu√© te dije sobre mis gustos?\")\n",
    "print(f\"ü§ñ Asistente: {response4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f7496",
   "metadata": {},
   "source": [
    "## Paso 7: Inspeccionar la Memoria üîç\n",
    "\n",
    "Veamos qu√© almacen√≥ la memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3be9f823",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã HISTORIAL COMPLETO EN MEMORIA:\n",
      "============================================================\n",
      "{'history': 'Human: Hola, me llamo Ana\\nAI: ¬°Hola, Ana! Es un placer conocerte. Soy tu asistente virtual.\\n\\n¬øEn qu√© puedo ayudarte hoy?\\nHuman: ¬øCu√°l es mi nombre?\\nAI: ¬°Tu nombre es Ana! Me lo dijiste hace un momento. üòä\\nHuman: Me gusta la m√∫sica rock\\nAI: ¬°Excelente gusto, Ana! El rock es un g√©nero con much√≠sima energ√≠a e historia. ü§ò\\n\\n¬øHay alguna banda o subg√©nero que te guste m√°s en particular?\\nHuman: ¬øQu√© te dije sobre mis gustos?\\nAI: Me dijiste que te gusta la m√∫sica rock. ¬°Un excelente gusto! ü§ò'}\n",
      "\n",
      "üìä Total de interacciones almacenadas: 0\n",
      "üí° ConversationBufferMemory almacena TODO sin l√≠mite\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìã HISTORIAL COMPLETO EN MEMORIA:\")\n",
    "print(\"=\"*60)\n",
    "print(buffer_memory.load_memory_variables({}))\n",
    "\n",
    "# Contar mensajes\n",
    "history = buffer_memory.load_memory_variables({})[\"history\"]\n",
    "num_interactions = history.count(\"Usuario:\") if history else 0\n",
    "print(f\"\\nüìä Total de interacciones almacenadas: {num_interactions}\")\n",
    "print(\"üí° ConversationBufferMemory almacena TODO sin l√≠mite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b9c356",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ü™ü EJERCICIO 2: ConversationBufferWindowMemory\n",
    "\n",
    "## ¬øQu√© es?\n",
    "\n",
    "**ConversationBufferWindowMemory** mantiene solo los **N mensajes m√°s recientes** (ventana deslizante).\n",
    "\n",
    "### Caracter√≠sticas:\n",
    "- ‚úÖ Controla el tama√±o de la memoria\n",
    "- ‚úÖ Uso de tokens predecible\n",
    "- ‚úÖ F√°cil de configurar (solo defines `k`)\n",
    "- ‚ö†Ô∏è Pierde contexto de mensajes antiguos\n",
    "\n",
    "### Caso de Uso Ideal:\n",
    "Conversaciones donde solo importan los **intercambios recientes** (ej: soporte t√©cnico, chat casual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a9bdbe",
   "metadata": {},
   "source": [
    "## Paso 8: Implementar ConversationBufferWindowMemory ü™ü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e96af3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ConversationBufferWindowMemory creada\n",
      "ü™ü Ventana: Solo √∫ltimas 3 interacciones (6 mensajes)\n"
     ]
    }
   ],
   "source": [
    "# Crear memoria con ventana de 3 interacciones (6 mensajes totales)\n",
    "window_memory = ConversationBufferWindowMemory(\n",
    "    k=3,  # N√∫mero de interacciones a recordar\n",
    "    memory_key=\"history\",\n",
    "    input_key=\"input\"\n",
    ")\n",
    "\n",
    "# Crear prompt template (mismo que antes)\n",
    "window_prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"],\n",
    "    template=\"\"\"\n",
    "Eres un asistente amigable y conversacional.\n",
    "\n",
    "Historial de conversaci√≥n:\n",
    "{history}\n",
    "\n",
    "Usuario: {input}\n",
    "Asistente:\"\"\"\n",
    ")\n",
    "\n",
    "# Crear cadena conversacional\n",
    "conversation_window = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=window_prompt,\n",
    "    memory=window_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ConversationBufferWindowMemory creada\")\n",
    "print(\"ü™ü Ventana: Solo √∫ltimas 3 interacciones (6 mensajes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341ff67a",
   "metadata": {},
   "source": [
    "## Paso 9: Probar ConversationBufferWindowMemory üß™\n",
    "\n",
    "Vamos a enviar **m√°s de 5 mensajes** para ver c√≥mo se descarta el historial antiguo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "932d7a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîµ PRUEBA 2: ConversationBufferWindowMemory (k=3)\n",
      "============================================================\n",
      "\n",
      "üí¨ Mensaje 1: Mi nombre es Carlos\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Eres un asistente amigable y conversacional.\n",
      "\n",
      "Historial de conversaci√≥n:\n",
      "Human: Mi nombre es Carlos\n",
      "AI: ¬°Hola Carlos! Es un placer conocerte. ¬øEn qu√© puedo ayudarte hoy?\n",
      "\n",
      "Usuario: Mi nombre es Carlos\n",
      "Asistente:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ü§ñ Asistente: ¬°Hola de nuevo, Carlos! ¬øEn qu√© puedo ayudarte hoy?\n",
      "üìä Interacciones en memoria: 0/3\n",
      "\n",
      "üí¨ Mensaje 2: Vivo en Madrid\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Eres un asistente amigable y conversacional.\n",
      "\n",
      "Historial de conversaci√≥n:\n",
      "Human: Mi nombre es Carlos\n",
      "AI: ¬°Hola Carlos! Es un placer conocerte. ¬øEn qu√© puedo ayudarte hoy?\n",
      "Human: Mi nombre es Carlos\n",
      "AI: ¬°Hola de nuevo, Carlos! ¬øEn qu√© puedo ayudarte hoy?\n",
      "\n",
      "Usuario: Vivo en Madrid\n",
      "Asistente:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ü§ñ Asistente: ¬°Ah, vives en Madrid! ¬°Qu√© ciudad tan vibrante y llena de vida!\n",
      "üìä Interacciones en memoria: 0/3\n",
      "\n",
      "üí¨ Mensaje 3: Trabajo como ingeniero\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Eres un asistente amigable y conversacional.\n",
      "\n",
      "Historial de conversaci√≥n:\n",
      "Human: Mi nombre es Carlos\n",
      "AI: ¬°Hola Carlos! Es un placer conocerte. ¬øEn qu√© puedo ayudarte hoy?\n",
      "Human: Mi nombre es Carlos\n",
      "AI: ¬°Hola de nuevo, Carlos! ¬øEn qu√© puedo ayudarte hoy?\n",
      "Human: Vivo en Madrid\n",
      "AI: ¬°Ah, vives en Madrid! ¬°Qu√© ciudad tan vibrante y llena de vida!\n",
      "\n",
      "Usuario: Trabajo como ingeniero\n",
      "Asistente:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ü§ñ Asistente: ¬°Ah, trabajas como ingeniero! ¬°Qu√© interesante! Es una profesi√≥n muy importante y llena de desaf√≠os.\n",
      "üìä Interacciones en memoria: 0/3\n",
      "\n",
      "üí¨ Mensaje 4: Me gusta el f√∫tbol\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Eres un asistente amigable y conversacional.\n",
      "\n",
      "Historial de conversaci√≥n:\n",
      "Human: Mi nombre es Carlos\n",
      "AI: ¬°Hola de nuevo, Carlos! ¬øEn qu√© puedo ayudarte hoy?\n",
      "Human: Vivo en Madrid\n",
      "AI: ¬°Ah, vives en Madrid! ¬°Qu√© ciudad tan vibrante y llena de vida!\n",
      "Human: Trabajo como ingeniero\n",
      "AI: ¬°Ah, trabajas como ingeniero! ¬°Qu√© interesante! Es una profesi√≥n muy importante y llena de desaf√≠os.\n",
      "\n",
      "Usuario: Me gusta el f√∫tbol\n",
      "Asistente:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ü§ñ Asistente: ¬°Ah, te gusta el f√∫tbol! ¬°Qu√© bien! Con la pasi√≥n que hay por el f√∫tbol en Madrid, ¬°seguro que lo disfrutas mucho!\n",
      "üìä Interacciones en memoria: 0/3\n",
      "\n",
      "üí¨ Mensaje 5: ¬øRecuerdas mi nombre?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Eres un asistente amigable y conversacional.\n",
      "\n",
      "Historial de conversaci√≥n:\n",
      "Human: Vivo en Madrid\n",
      "AI: ¬°Ah, vives en Madrid! ¬°Qu√© ciudad tan vibrante y llena de vida!\n",
      "Human: Trabajo como ingeniero\n",
      "AI: ¬°Ah, trabajas como ingeniero! ¬°Qu√© interesante! Es una profesi√≥n muy importante y llena de desaf√≠os.\n",
      "Human: Me gusta el f√∫tbol\n",
      "AI: ¬°Ah, te gusta el f√∫tbol! ¬°Qu√© bien! Con la pasi√≥n que hay por el f√∫tbol en Madrid, ¬°seguro que lo disfrutas mucho!\n",
      "\n",
      "Usuario: ¬øRecuerdas mi nombre?\n",
      "Asistente:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ü§ñ Asistente: No, no me has dicho tu nombre. ¬°Me encantar√≠a saberlo si quieres compart√≠rmelo! üòä\n",
      "üìä Interacciones en memoria: 0/3\n",
      "\n",
      "üí¨ Mensaje 6: ¬øRecuerdas mi trabajo?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Eres un asistente amigable y conversacional.\n",
      "\n",
      "Historial de conversaci√≥n:\n",
      "Human: Trabajo como ingeniero\n",
      "AI: ¬°Ah, trabajas como ingeniero! ¬°Qu√© interesante! Es una profesi√≥n muy importante y llena de desaf√≠os.\n",
      "Human: Me gusta el f√∫tbol\n",
      "AI: ¬°Ah, te gusta el f√∫tbol! ¬°Qu√© bien! Con la pasi√≥n que hay por el f√∫tbol en Madrid, ¬°seguro que lo disfrutas mucho!\n",
      "Human: ¬øRecuerdas mi nombre?\n",
      "AI: No, no me has dicho tu nombre. ¬°Me encantar√≠a saberlo si quieres compart√≠rmelo! üòä\n",
      "\n",
      "Usuario: ¬øRecuerdas mi trabajo?\n",
      "Asistente:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ü§ñ Asistente: ¬°S√≠, claro que s√≠! Eres ingeniero/a. üòä Lo mencionaste al principio.\n",
      "üìä Interacciones en memoria: 0/3\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üîµ PRUEBA 2: ConversationBufferWindowMemory (k=3)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "mensajes = [\n",
    "    \"Mi nombre es Carlos\",\n",
    "    \"Vivo en Madrid\",\n",
    "    \"Trabajo como ingeniero\",\n",
    "    \"Me gusta el f√∫tbol\",\n",
    "    \"¬øRecuerdas mi nombre?\",  # Deber√≠a haberlo olvidado\n",
    "    \"¬øRecuerdas mi trabajo?\",  # Deber√≠a recordarlo\n",
    "]\n",
    "\n",
    "for i, msg in enumerate(mensajes, 1):\n",
    "    print(f\"\\nüí¨ Mensaje {i}: {msg}\")\n",
    "    response = conversation_window.predict(input=msg)\n",
    "    print(f\"ü§ñ Asistente: {response}\")\n",
    "    \n",
    "    # Mostrar cantidad de mensajes en memoria\n",
    "    history = window_memory.load_memory_variables({})[\"history\"]\n",
    "    num_in_memory = history.count(\"Usuario:\") if history else 0\n",
    "    print(f\"üìä Interacciones en memoria: {num_in_memory}/{window_memory.k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020fe512",
   "metadata": {},
   "source": [
    "## Paso 10: Verificar P√©rdida de Contexto üîç\n",
    "\n",
    "Observa c√≥mo el modelo **olvida** informaci√≥n que sali√≥ de la ventana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3daa9775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã CONTENIDO ACTUAL DE LA VENTANA:\n",
      "============================================================\n",
      "{'history': 'Human: Me gusta el f√∫tbol\\nAI: ¬°Ah, te gusta el f√∫tbol! ¬°Qu√© bien! Con la pasi√≥n que hay por el f√∫tbol en Madrid, ¬°seguro que lo disfrutas mucho!\\nHuman: ¬øRecuerdas mi nombre?\\nAI: No, no me has dicho tu nombre. ¬°Me encantar√≠a saberlo si quieres compart√≠rmelo! üòä\\nHuman: ¬øRecuerdas mi trabajo?\\nAI: ¬°S√≠, claro que s√≠! Eres ingeniero/a. üòä Lo mencionaste al principio.'}\n",
      "\n",
      "üí° AN√ÅLISIS:\n",
      "‚Ä¢ Los primeros mensajes (nombre, ciudad) probablemente fueron olvidados\n",
      "‚Ä¢ Solo se conservan los √∫ltimos 5 intercambios (10 mensajes)\n",
      "‚Ä¢ Ventaja: Uso de tokens controlado\n",
      "‚Ä¢ Desventaja: P√©rdida de contexto antiguo\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìã CONTENIDO ACTUAL DE LA VENTANA:\")\n",
    "print(\"=\"*60)\n",
    "print(window_memory.load_memory_variables({}))\n",
    "\n",
    "print(\"\\nüí° AN√ÅLISIS:\")\n",
    "print(\"‚Ä¢ Los primeros mensajes (nombre, ciudad) probablemente fueron olvidados\")\n",
    "print(\"‚Ä¢ Solo se conservan los √∫ltimos 5 intercambios (10 mensajes)\")\n",
    "print(\"‚Ä¢ Ventaja: Uso de tokens controlado\")\n",
    "print(\"‚Ä¢ Desventaja: P√©rdida de contexto antiguo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0e6cc6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìã EJERCICIO 3: ConversationSummaryMemory\n",
    "\n",
    "## ¬øQu√© es?\n",
    "\n",
    "**ConversationSummaryMemory** usa un **LLM para resumir** las conversaciones anteriores en lugar de almacenar todo.\n",
    "\n",
    "### Caracter√≠sticas:\n",
    "- ‚úÖ Mantiene informaci√≥n clave de toda la conversaci√≥n\n",
    "- ‚úÖ Reduce tokens significativamente\n",
    "- ‚úÖ Ideal para conversaciones muy largas\n",
    "- ‚ö†Ô∏è Requiere llamadas adicionales al LLM para resumir\n",
    "- ‚ö†Ô∏è Puede perder detalles espec√≠ficos\n",
    "\n",
    "### Caso de Uso Ideal:\n",
    "Conversaciones **largas** donde necesitas preservar el contexto general (ej: sesiones terap√©uticas, consultor√≠a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28976f4",
   "metadata": {},
   "source": [
    "## Paso 11: Implementar ConversationSummaryMemory üìã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "508bab9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ConversationSummaryMemory creada\n",
      "üìã Tipo: Genera res√∫menes autom√°ticos de la conversaci√≥n\n",
      "‚öôÔ∏è Usa el LLM para comprimir informaci√≥n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2327/241849634.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  summary_memory = ConversationSummaryMemory(\n"
     ]
    }
   ],
   "source": [
    "# Crear memoria con res√∫menes autom√°ticos\n",
    "summary_memory = ConversationSummaryMemory(\n",
    "    llm=llm,  # Necesita un LLM para generar res√∫menes\n",
    "    memory_key=\"history\",\n",
    "    input_key=\"input\"\n",
    ")\n",
    "\n",
    "# Crear prompt template\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"],\n",
    "    template=\"\"\"\n",
    "Eres un asistente amigable y conversacional.\n",
    "\n",
    "Resumen de la conversaci√≥n:\n",
    "{history}\n",
    "\n",
    "Usuario: {input}\n",
    "Asistente:\"\"\"\n",
    ")\n",
    "\n",
    "# Crear cadena conversacional\n",
    "conversation_summary = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=summary_prompt,\n",
    "    memory=summary_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ConversationSummaryMemory creada\")\n",
    "print(\"üìã Tipo: Genera res√∫menes autom√°ticos de la conversaci√≥n\")\n",
    "print(\"‚öôÔ∏è Usa el LLM para comprimir informaci√≥n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6364ad1",
   "metadata": {},
   "source": [
    "## Paso 12: Probar ConversationSummaryMemory üß™\n",
    "\n",
    "Vamos a tener una conversaci√≥n larga y ver c√≥mo se resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b6c9111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîµ PRUEBA 3: ConversationSummaryMemory\n",
      "============================================================\n",
      "\n",
      "üí¨ Mensaje 1: Hola, soy Mar√≠a y tengo 28 a√±os\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Eres un asistente amigable y conversacional.\n",
      "\n",
      "Resumen de la conversaci√≥n:\n",
      "Current summary:\n",
      "Mar√≠a introduces herself as a 28-year-old graphic designer working in a startup. The AI greets her, expresses pleasure in meeting her, offers assistance, and praises her creative and energetic job. Mar√≠a then reveals she moved to Barcelona from Argentina two years ago. The AI finds this an incredible life experience, noting Barcelona's vibrant and creative nature is perfect for a graphic designer, and asks about her adaptation to the city and if she feels at home. Mar√≠a shares that she is learning Catalan, in addition to speaking Spanish and English. The AI praises this initiative, calling it a wonderful way to further immerse herself in Barcelona's culture and feel truly at home, and offers to help her practice Catalan (or Spanish/English) if she needs it. The AI then asks about her progress with learning Catalan and if she finds it very different from Spanish or if there are helpful similarities.\n",
      "\n",
      "Usuario: Hola, soy Mar√≠a y tengo 28 a√±os\n",
      "Asistente:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n",
      "Please retry in 44.763735585s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 44\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n",
      "Please retry in 42.724322257s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 42\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n",
      "Please retry in 38.877744739s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 38\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n",
      "Please retry in 31.37803093s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 31\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\n",
      "Please retry in 16.603966565s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 16\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\nPlease retry in 47.299342148s. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 20\n}\n, retry_delay {\n  seconds: 47\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceExhausted\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, msg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(conversacion_larga, \u001b[32m1\u001b[39m):\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müí¨ Mensaje \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     response = \u001b[43mconversation_summary\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mü§ñ Asistente: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m     time.sleep(\u001b[32m4\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/langchain/chains/llm.py:325\u001b[39m, in \u001b[36mLLMChain.predict\u001b[39m\u001b[34m(self, callbacks, **kwargs)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks = \u001b[38;5;28;01mNone\u001b[39;00m, **kwargs: Any) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    311\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[32m    312\u001b[39m \n\u001b[32m    313\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    323\u001b[39m \u001b[33;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m.output_key]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/langchain_core/_api/deprecation.py:193\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    191\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    192\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/langchain/chains/base.py:410\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    379\u001b[39m \n\u001b[32m    380\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    401\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    402\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    403\u001b[39m config = {\n\u001b[32m    404\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    405\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    406\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    407\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    408\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/langchain/chains/base.py:165\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    164\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    167\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    168\u001b[39m     )\n\u001b[32m    170\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    171\u001b[39m         inputs,\n\u001b[32m    172\u001b[39m         outputs,\n\u001b[32m    173\u001b[39m         return_only_outputs,\n\u001b[32m    174\u001b[39m     )\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/langchain/chains/llm.py:127\u001b[39m, in \u001b[36mLLMChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call\u001b[39m(\n\u001b[32m    123\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    124\u001b[39m     inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m    125\u001b[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    126\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.create_outputs(response)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/langchain/chains/llm.py:139\u001b[39m, in \u001b[36mLLMChain.generate\u001b[39m\u001b[34m(self, input_list, run_manager)\u001b[39m\n\u001b[32m    137\u001b[39m callbacks = run_manager.get_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm, BaseLanguageModel):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m results = \u001b[38;5;28mself\u001b[39m.llm.bind(stop=stop, **\u001b[38;5;28mself\u001b[39m.llm_kwargs).batch(\n\u001b[32m    146\u001b[39m     cast(\u001b[38;5;28mlist\u001b[39m, prompts),\n\u001b[32m    147\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks},\n\u001b[32m    148\u001b[39m )\n\u001b[32m    149\u001b[39m generations: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Generation]] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py:1025\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     **kwargs: Any,\n\u001b[32m   1023\u001b[39m ) -> LLMResult:\n\u001b[32m   1024\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py:842\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    840\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    841\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m842\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    848\u001b[39m         )\n\u001b[32m    849\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    850\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py:1091\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1089\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1095\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py:1790\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   1788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmax_retries\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m   1789\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mmax_retries\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.max_retries\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m response: GenerateContentResponse = \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1792\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1795\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1796\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py:238\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    231\u001b[39m params = (\n\u001b[32m    232\u001b[39m     {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _allowed_params_prediction_service}\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (request := kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   (...)\u001b[39m\u001b[32m    236\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m kwargs\n\u001b[32m    237\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py:208\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_with_retry\u001b[39m(**kwargs: Any) -> Any:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m FailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    210\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mlocation is not supported\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m exc.message:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:869\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    866\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    868\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    167\u001b[39m     time.sleep(next_sleep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     on_error_fn(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/google/api_core/timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py:77\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(*args, **kwargs)\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mResourceExhausted\u001b[39m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\nPlease retry in 47.299342148s. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 20\n}\n, retry_delay {\n  seconds: 47\n}\n]"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üîµ PRUEBA 3: ConversationSummaryMemory\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "conversacion_larga = [\n",
    "    \"Hola, soy Mar√≠a y tengo 28 a√±os\",\n",
    "    \"Trabajo como dise√±adora gr√°fica en una startup\",\n",
    "    \"Me mud√© a Barcelona hace 2 a√±os desde Argentina\",\n",
    "    \"Estoy aprendiendo catal√°n y ya hablo espa√±ol e ingl√©s\"\n",
    "]\n",
    "\n",
    "for i, msg in enumerate(conversacion_larga, 1):\n",
    "    print(f\"\\nüí¨ Mensaje {i}: {msg}\")\n",
    "    response = conversation_summary.predict(input=msg)\n",
    "    print(f\"ü§ñ Asistente: {response}\")\n",
    "    time.sleep(12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e87e693",
   "metadata": {},
   "source": [
    "## Paso 13: Inspeccionar el Resumen Generado üîç\n",
    "\n",
    "Veamos c√≥mo la memoria resumi√≥ toda la conversaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72d3429f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã RESUMEN AUTOM√ÅTICO DE LA CONVERSACI√ìN:\n",
      "============================================================\n",
      "{'history': \"Current summary:\\nMar√≠a introduces herself as a 28-year-old graphic designer working in a startup. The AI greets her, expresses pleasure in meeting her, offers assistance, and praises her creative and energetic job. Mar√≠a then reveals she moved to Barcelona from Argentina two years ago. The AI finds this an incredible life experience, noting Barcelona's vibrant and creative nature is perfect for a graphic designer, and asks about her adaptation to the city and if she feels at home. Mar√≠a shares that she is learning Catalan, in addition to speaking Spanish and English. The AI praises this initiative, calling it a wonderful way to further immerse herself in Barcelona's culture and feel truly at home, and offers to help her practice Catalan (or Spanish/English) if she needs it. The AI then asks about her progress with learning Catalan and if she finds it very different from Spanish or if there are helpful similarities.\"}\n",
      "\n",
      "üí° AN√ÅLISIS:\n",
      "‚Ä¢ El LLM gener√≥ un resumen condensado de toda la conversaci√≥n\n",
      "‚Ä¢ Mantiene informaci√≥n clave (nombre, profesi√≥n, ubicaci√≥n, idiomas)\n",
      "‚Ä¢ Reduce significativamente el n√∫mero de tokens\n",
      "‚Ä¢ Ideal para conversaciones de 20+ mensajes\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìã RESUMEN AUTOM√ÅTICO DE LA CONVERSACI√ìN:\")\n",
    "print(\"=\"*60)\n",
    "memory_content = summary_memory.load_memory_variables({})\n",
    "print(memory_content)\n",
    "\n",
    "print(\"\\nüí° AN√ÅLISIS:\")\n",
    "print(\"‚Ä¢ El LLM gener√≥ un resumen condensado de toda la conversaci√≥n\")\n",
    "print(\"‚Ä¢ Mantiene informaci√≥n clave (nombre, profesi√≥n, ubicaci√≥n, idiomas)\")\n",
    "print(\"‚Ä¢ Reduce significativamente el n√∫mero de tokens\")\n",
    "print(\"‚Ä¢ Ideal para conversaciones de 20+ mensajes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429c4a5a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìä COMPARACI√ìN DE LOS 3 TIPOS DE MEMORIA\n",
    "\n",
    "## Tabla Comparativa\n",
    "\n",
    "| Tipo de Memoria | Completud | Uso Tokens | Dificultad | Caso Ideal |\n",
    "|---|---|---|---|---|\n",
    "| **ConversationBufferMemory** üìù | 100% | Alto (crece ‚àû) | üü¢ F√°cil | Conversaciones cortas (5-10 msgs) |\n",
    "| **ConversationBufferWindowMemory** ü™ü | Parcial (solo ventana) | Controlado (fijo) | üü¢ F√°cil | Solo contexto reciente |\n",
    "| **ConversationSummaryMemory** üìã | Alto (resumido) | Bajo | üü° Medio | Conversaciones largas (20+ msgs) |\n",
    "\n",
    "## Recomendaciones\n",
    "\n",
    "‚úÖ **Buffer**: Usa cuando necesitas TODO el contexto y la conversaci√≥n es corta\n",
    "\n",
    "‚úÖ **Window**: Usa cuando solo importan los √∫ltimos N mensajes\n",
    "\n",
    "‚úÖ **Summary**: Usa cuando la conversaci√≥n es larga pero necesitas mantener contexto\n",
    "\n",
    "‚ö†Ô∏è **Hybrid**: Para casos complejos, combina Window + Summary (ConversationSummaryBufferMemory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a928e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ EJERCICIO PR√ÅCTICO: Elige la Memoria Adecuada\n",
    "\n",
    "## Escenarios\n",
    "\n",
    "Para cada escenario, decide qu√© tipo de memoria usar y por qu√©:\n",
    "\n",
    "### Escenario 1: Bot de Soporte T√©cnico\n",
    "- Usuario reporta un problema\n",
    "- Conversaci√≥n t√≠pica: 5-8 mensajes\n",
    "- Necesita recordar el problema inicial\n",
    "\n",
    "**üí° Tu respuesta:** _____________________\n",
    "\n",
    "### Escenario 2: Asistente de Terapia\n",
    "- Sesiones de 30-60 minutos\n",
    "- 40-100 intercambios por sesi√≥n\n",
    "- Necesita recordar informaci√≥n clave de toda la sesi√≥n\n",
    "\n",
    "**üí° Tu respuesta:** _____________________\n",
    "\n",
    "### Escenario 3: Chat Casual\n",
    "- Conversaci√≥n informal\n",
    "- Solo importa el contexto inmediato (√∫ltimas 3-4 respuestas)\n",
    "- No necesita recordar el inicio\n",
    "\n",
    "**üí° Tu respuesta:** _____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29e899e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üèÜ DESAF√çO FINAL: Crea Tu Propio Chatbot\n",
    "\n",
    "## Tarea\n",
    "\n",
    "Crea un chatbot que:\n",
    "1. Use **ConversationBufferWindowMemory** con k=4\n",
    "2. Tenga una personalidad espec√≠fica (ej: chef, entrenador, tutor)\n",
    "3. Mantenga conversaciones coherentes\n",
    "\n",
    "### Plantilla de C√≥digo\n",
    "\n",
    "```python\n",
    "# Paso 1: Crear memoria\n",
    "mi_memoria = ConversationBufferWindowMemory(\n",
    "    k=4,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Paso 2: Crear prompt personalizado\n",
    "mi_prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"],\n",
    "    template=\"\"\"\n",
    "    Eres un [TU_PERSONALIDAD_AQU√ç].\n",
    "    \n",
    "    Conversaci√≥n previa:\n",
    "    {history}\n",
    "    \n",
    "    Usuario: {input}\n",
    "    Asistente:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Paso 3: Crear conversaci√≥n\n",
    "mi_chatbot = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=mi_memoria,\n",
    "    prompt=mi_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Paso 4: Probar\n",
    "respuesta = mi_chatbot.predict(input=\"Hola, ¬øc√≥mo est√°s?\")\n",
    "print(respuesta)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfa987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ TU C√ìDIGO AQU√ç\n",
    "# Crea tu chatbot personalizado\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8609d24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéâ ¬°Felicitaciones!\n",
    "\n",
    "Has completado el taller de **Memoria Conversacional con LangChain**.\n",
    "\n",
    "## üìö Lo que aprendiste:\n",
    "\n",
    "‚úÖ Implementar ConversationBufferMemory para conversaciones cortas\n",
    "\n",
    "‚úÖ Usar ConversationBufferWindowMemory para contexto reciente\n",
    "\n",
    "‚úÖ Aplicar ConversationSummaryMemory para conversaciones largas\n",
    "\n",
    "‚úÖ Elegir el tipo de memoria seg√∫n el caso de uso\n",
    "\n",
    "## üöÄ Pr√≥ximos Pasos:\n",
    "\n",
    "- Explora **ConversationSummaryBufferMemory** (h√≠brida)\n",
    "- Integra memoria con **RAG** (Retrieval Augmented Generation)\n",
    "- Implementa memoria **persistente** con bases de datos\n",
    "- Aprende sobre **personalidad din√°mica** en LLMs\n",
    "\n",
    "---\n",
    "\n",
    "**Desarrollado para el Semillero de IA** üß†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d541b3-0292-49bd-8e7b-476c5d06dce1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
